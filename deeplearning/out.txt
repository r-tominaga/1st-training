こう なる と 、 CNN は NLP の タスク に は うまく 適合 し ない ん じゃ ない か という 気 が し て き ます 。 リカレントニューラルネットワーク は もっと 直感 的 です し 、 実際 に 人間 が 言語 を 処理 する やり方 に 似 て い ます 。 左 から 右 に 順番 に 読ん で いく 方法 です ね 。 と は 言っ て も 、 これ まで の 説明 は 決して CNN が うまく 動か ない という 意味 で は あり ませ ん 。 完璧 な モデル など 無い が 、 それでも 役に立つ モデル は ある という 言葉 が あり ます が 、 NLP の 問題 へ 適用 さ れ た CNN は 、 結果 的 に は とても よい 性能 を 発揮 し ます 。 単純 な Bag of Words モデル は 誤っ た 仮定 の もと 単純 化 し すぎ な の は 明らか です が 、 に も かかわら ず しばらく は 一般 的 な アプローチ で あり 、 人々 は それ を 使っ て より 良い 結果 を 得よ う と し て き まし た 。 CNN を 使う 言い分 として は 、 とても 速い という こと です 。 非常 に 速い です 。 畳み込み は コンピューターグラフィックス において 重要 な もの で あり GPU 上 に ハードウェア レベル で 実装 さ れ て い ます 。 n - grams の よう な もの と 比べ て CNN は 効率 的 に 単語 を 表現 が でき ます 。 ボキャブラリー が 巨大 な 場合 、 3 - grams 以上 の もの は 計算 量 が 多 すぎ て すぐ に 計算 でき なく なり ます 。 Google で さえ 5 - grams を 超える もの は 提供 し て い ませ ん 。 畳み込み フィルタ は ボキャブラリー 全体 を 表現 する 必要 なく 、 勝手 に 適切 な 表現 を 学習 し て くれ ます 。 最初 の 層 に ある たくさん の 学習 済み の フィルタ は n - grams と 非常 に 良く 似 た 特徴 を 捉え ます が 、 より コンパクト な 表現 を 得る こと が できる と 考える と 良い でしょ う ( しかも 計算 量 が 多 すぎ て 計算 が でき ない といった 制限 も あり ませ ん )。 CNN を NLP へ 適用 する やり方 を 説明 する 前 に 、 CNN を 構築 する 際 に 必要 と なっ て くる もの が いくつ か あり ます ので それ を 見 て いき ましょ う 。 きっと CNN 理解 の 手助け と なる はず です 。 私 が 最初 に 畳み込み の 説明 を し た 時 、 フィルタ を 適用 する 際 の 詳細 について 説明 を 飛ばし た もの が あり ます 。 行列 の 真ん中 辺り に 3 x 3 の フィルタ を 適用 する の は 問題 あり ませ ん が 、 それでは フチ の 辺り に 適用 する 場合 は どう でしょ う か ？ 行列 の 左側 に も 上側 に も 隣接 し た 要素 が ない よう な 、 たとえば 行列 の 最初 の 要素 に は どう やっ て フィルタ を 適用 すれ ば よい でしょ う か ？ そういった 場合 に は 、 ゼロパディング が 使え ます 。 行列 の 外側 に はみ出し て しまう 要素 は 全て 0 で 埋める の です 。 こう する こと で 、 入力 と なる 行列 の 全 要素 にわたって フィルタ を 適用 する こと が でき ます 。 ゼロパディング を 行う こと は wide convolution と も 呼ば れ 、 逆 に ゼロパディング を し ない 場合 は narrow convolution と 呼ば れ ます 。 1 次元 で の 例 を 見 て み ましょ う 入力 データ の サイズ に対して フィルタ サイズ が 大きい 時 に は wide convolution が 有用 です 。 ストライド という 畳み込み のも う 一つ の ハイパーパラメータ が あり ます 。 これ は フィルタ を 順に 適用 し て いく 際 に 、 フィルタ を どれ くらい シフト する の か という 値 です 。 これ まで に 示し て き た 例 は 全て ストライド 1 で 、 フィルタ は 重複 し ながら 連続 的 に 適用 さ れ て い ます 。 ストライド を 大きく する と フィルタ の 適用 回数 は 少なく なっ て 、 出力 の サイズ も 小さく なり ます 。 以下 の よう な 図 が Stanford cs 231 に あり ます が 、 これ は 1 次元 の 入力 に対して 、 ストライド の サイズ が 1 または 2 の フィルタ を 適用 し て いる 様子 です 。 畳み込み ニューラルネットワーク の 鍵 は 、 畳み込み 層 の 後 に 適用 さ れる プーリング 層 です 。 プーリング 層 は 、 入力 を サブ サンプリング し ます 。 最も 良く 使わ れる プーリング は 、 各 フィルタ の 結果 の 中 から 最大 値 を 得る 操作 です 。 ただ 、 畳み込み 結果 の 行列 全体 にわたって プーリング する 必要 は なく 、 指定 サイズ の ウィンドウ 上 で プーリング する こと も でき ます 。 たとえば 、 以下 の 図 は 2 x 2 の サイズ の ウィンドウ 上 で 最大 プーリング を 実行 し た 様子 です ( NLP で は 一般 的 に 出力 全体 にわたって プーリング を 適用 し ます 。 つまり 各 フィルタ から は 1 つ の 数値 が 出力 さ れる こと に なり ます )。 プーリング 層 を はさむ 理由 は いくつ か あり ます 。 プーリング の 特徴 の 1 つ は 、 出力 さ れる 行列 が 固定 サイズ に なる という こと です 。 たとえば 1000 個 の フィルタ が あっ て それぞれ の フィルタ に対して 最大 プーリング を 適用 し た と する と 、 入力 の サイズ や フィルタ の サイズ が どんな もの で あっ て も 結果 として は 1000 次元 の 出力 が 得 られ ます ね 。 これ は つまり 、 文章 の サイズ や フィルタ の サイズ が 可変 だっ た として も 、 最終 的 に 分類 器 へ データ が 渡っ て くる 時点 で は 常に 同じ 次元 に なっ て いる という こと です 。 また 、 プーリング は 次元 削減 も 行い ます が 、 単に 次元 を 削減 する の で は なく 必要 な 情報 は 維持 し た まま 次元 を 削減 し て くれ ます 。 フィルタ を ある 特定 の 特徴 を 抽出 する ため の もの として 考える の です 。 たとえば “ not amazing ” など の 否定 が 文章 内 に 含ま れ て いる か どう か を 検出 する ため の もの 、 という 感じ です 。 もし こんな フレーズ が 文章 の どこ か に で て き た 場合 、 その 部分 に フィルタ を 適用 する と 、 出力 さ れる 計算 結果 は 大きな 値 に なる でしょ う 。 しかし 、 それ 以外 の 別 の 部分 に フィルタ を 適用 し た 場合 は 、 出力 結果 は 小さな 値 に なる でしょ う 。 最大 プーリング を 適用 する こと で 、 文章 中 に “ とある 特徴 ” が 存在 する か どう か という 情報 は 残っ た まま です が 、 その 特徴 が 実際 に どこ に 出現 する の か といった 情報 は 失わ れる こと に なり ます 。 しかし 、 この よう な 位置 に関する 情報 は 本当に 消え て も 大丈夫 な の でしょ う か ？ 答え は yes です 。 n - grams モデル も 似 た よう な もの です 。 位置 に関する 情報 は 失い ます が 、 フィルタ によって 捉え られ た 局所 的 な 情報 - たとえば “ not amazing ” と “ amazing not ” の 違い など - は 残っ た まま な の です 。 で は 、 主 に 感情 分析 と カテゴリ 分類 から 成る 様々 な データセット 上 で CNN の アーキテクチャ を 評価 し て い ます 。 CNN は 全体 的 に とても 良い パフォーマンス を 発揮 し て い ます 。 この 論文 で 使わ れ て いる ネットワーク は 非常 に シンプル な のに 、 とても 強力 な ので 驚き です 。 入力 層 は word 2 vec による 単語 埋め込み 表現 で 構成 さ れ た 文章 で 、 その後 に 複数 の フィルタ を 持つ 畳み込み 層 と 最大 プーリング 層 が 続き 、 そして 最後 に softmax 分類 器 が あり ます 。 この 論文 で は 、 一方 は 学習 中 に 微 修正 さ れ て いく 動的 な 単語 埋め込み 表現 、 もう 一方 は 学習 中 に 変化 し ない 静的 な 単語 埋め込み 表現 、 といった 2 つ の 異なる チャンネル を 持つ データ に対する 実験 も し て い ます 。 似 て はい ます が 、 もう少し 複雑 な アーキテクチャ が [ 2 ] で 提案 さ れ て い ます 。 [ 6 ] で は “ semantic clustering ” と 呼ば れる 操作 を 行う 層 を ネットワーク に 追加 し て い ます 。 CNN を 実装 する ため に は 、 いろいろ な ハイパーパラメータ を 決める 必要 が あり ます 。 いくつ か は 先ほど 紹介 し まし た が 、 入力 データ の ベクトル 表現 ( word 2 vec な の か GloVe な の か one - hot な の か )、 その 数 、 畳み込み フィルタ の サイズ 、 プーリング の 方法 ( 最大 プーリング な の か 平均 プーリング な の か )、 活性 化 関数 ( ReLU な の か tanh な の か )、 など です 。 [ 7 ] で は CNN の ハイパーパラメータ を 様々 に 変化 さ せ ながら 、 その 時 の CNN の パフォーマンス と 複数 回 実行 し た 際 の 分散 を 調査 し 、 評価 し て い ます 。 もし 、 あなた が 自分 で テキスト 分類 問題 を 解く ため の CNN を 実装 する つもり で あれ ば 、 この 論文 の 結果 を 初期 値 として 使う の が 良い でしょ う 。 この 論文 に よる と 、 平均 プーリング より 最大 プーリング の 方 が 毎回 いい 結果 を 出し て おり 、 理想 的 な フィルタ サイズ を 考える の は とても 重要 です が それ は タスク 毎 に 異なっ て い ます 。 それから 、 正則 化 項 を 導入 し て も NLP において は 結果 が 大きく 変わる こと は ない よう です 。 1 つ 注意 点 として は 、 この 研究 で 使っ て いる データセット は どれ も テキスト の 長 さ が 非常 に 似 て いる もの ばかり です ので 、 テキスト 長 が 明らか に 異なる よう な データ に対して は 同じ よう に 考える こと は 恐らく でき ない でしょ う 。
