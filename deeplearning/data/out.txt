こう なる と 、 CNN は NLP の タスク に は うまく 適合 し ない ん じゃ ない か という 気 が し て き ます 。 リカレントニューラルネットワーク は もっと 直感 的 です し 、 実際 に 人間 が 言語 を 処理 する やり方 に 似 て い ます 。 左 から 右 に 順番 に 読ん で いく 方法 です ね 。 と は 言っ て も 、 これ まで の 説明 は 決して CNN が うまく 動か ない という 意味 で は あり ませ ん 。 完璧 な モデル など 無い が 、 それでも 役に立つ モデル は ある という 言葉 が あり ます が 、 NLP の 問題 へ 適用 さ れ た CNN は 、 結果 的 に は とても よい 性能 を 発揮 し ます 。 単純 な Bag of Words モデル は 誤っ た 仮定 の もと 単純 化 し すぎ な の は 明らか です が 、 に も かかわら ず しばらく は 一般 的 な アプローチ で あり 、 人々 は それ を 使っ て より 良い 結果 を 得よ う と し て き まし た 。 CNN を 使う 言い分 として は 、 とても 速い という こと です 。 非常 に 速い です 。 畳み込み は コンピューターグラフィックス において 重要 な もの で あり GPU 上 に ハードウェア レベル で 実装 さ れ て い ます 。 n - grams の よう な もの と 比べ て CNN は 効率 的 に 単語 を 表現 が でき ます 。 ボキャブラリー が 巨大 な 場合 、 3 - grams 以上 の もの は 計算 量 が 多 すぎ て すぐ に 計算 でき なく なり ます 。 Google で さえ 5 - grams を 超える もの は 提供 し て い ませ ん 。 畳み込み フィルタ は ボキャブラリー 全体 を 表現 する 必要 なく 、 勝手 に 適切 な 表現 を 学習 し て くれ ます 。 最初 の 層 に ある たくさん の 学習 済み の フィルタ は n - grams と 非常 に 良く 似 た 特徴 を 捉え ます が 、 より コンパクト な 表現 を 得る こと が できる と 考える と 良い でしょ う ( しかも 計算 量 が 多 すぎ て 計算 が でき ない といった 制限 も あり ませ ん )。 CNN を NLP へ 適用 する やり方 を 説明 する 前 に 、 CNN を 構築 する 際 に 必要 と なっ て くる もの が いくつ か あり ます ので それ を 見 て いき ましょ う 。 きっと CNN 理解 の 手助け と なる はず です 。 私 が 最初 に 畳み込み の 説明 を し た 時 、 フィルタ を 適用 する 際 の 詳細 について 説明 を 飛ばし た もの が あり ます 。 行列 の 真ん中 辺り に 3 x 3 の フィルタ を 適用 する の は 問題 あり ませ ん が 、 それでは フチ の 辺り に 適用 する 場合 は どう でしょ う か ？ 行列 の 左側 に も 上側 に も 隣接 し た 要素 が ない よう な 、 たとえば 行列 の 最初 の 要素 に は どう やっ て フィルタ を 適用 すれ ば よい でしょ う か ？ そういった 場合 に は 、 ゼロパディング が 使え ます 。 行列 の 外側 に はみ出し て しまう 要素 は 全て 0 で 埋める の です 。 こう する こと で 、 入力 と なる 行列 の 全 要素 にわたって フィルタ を 適用 する こと が でき ます 。 ゼロパディング を 行う こと は wide convolution と も 呼ば れ 、 逆 に ゼロパディング を し ない 場合 は narrow convolution と 呼ば れ ます 。 1 次元 で の 例 を 見 て み ましょ う 入力 データ の サイズ に対して フィルタ サイズ が 大きい 時 に は wide convolution が 有用 です 。 ストライド という 畳み込み のも う 一つ の ハイパーパラメータ が あり ます 。 これ は フィルタ を 順に 適用 し て いく 際 に 、 フィルタ を どれ くらい シフト する の か という 値 です 。 これ まで に 示し て き た 例 は 全て ストライド 1 で 、 フィルタ は 重複 し ながら 連続 的 に 適用 さ れ て い ます 。 ストライド を 大きく する と フィルタ の 適用 回数 は 少なく なっ て 、 出力 の サイズ も 小さく なり ます 。 以下 の よう な 図 が Stanford cs 231 に あり ます が 、 これ は 1 次元 の 入力 に対して 、 ストライド の サイズ が 1 または 2 の フィルタ を 適用 し て いる 様子 です 。 畳み込み ニューラルネットワーク の 鍵 は 、 畳み込み 層 の 後 に 適用 さ れる プーリング 層 です 。 プーリング 層 は 、 入力 を サブ サンプリング し ます 。 最も 良く 使わ れる プーリング は 、 各 フィルタ の 結果 の 中 から 最大 値 を 得る 操作 です 。 ただ 、 畳み込み 結果 の 行列 全体 にわたって プーリング する 必要 は なく 、 指定 サイズ の ウィンドウ 上 で プーリング する こと も でき ます 。 たとえば 、 以下 の 図 は 2 x 2 の サイズ の ウィンドウ 上 で 最大 プーリング を 実行 し た 様子 です ( NLP で は 一般 的 に 出力 全体 にわたって プーリング を 適用 し ます 。 つまり 各 フィルタ から は 1 つ の 数値 が 出力 さ れる こと に なり ます )。 プーリング 層 を はさむ 理由 は いくつ か あり ます 。 プーリング の 特徴 の 1 つ は 、 出力 さ れる 行列 が 固定 サイズ に なる という こと です 。 たとえば 1000 個 の フィルタ が あっ て それぞれ の フィルタ に対して 最大 プーリング を 適用 し た と する と 、 入力 の サイズ や フィルタ の サイズ が どんな もの で あっ て も 結果 として は 1000 次元 の 出力 が 得 られ ます ね 。 これ は つまり 、 文章 の サイズ や フィルタ の サイズ が 可変 だっ た として も 、 最終 的 に 分類 器 へ データ が 渡っ て くる 時点 で は 常に 同じ 次元 に なっ て いる という こと です 。 また 、 プーリング は 次元 削減 も 行い ます が 、 単に 次元 を 削減 する の で は なく 必要 な 情報 は 維持 し た まま 次元 を 削減 し て くれ ます 。 フィルタ を ある 特定 の 特徴 を 抽出 する ため の もの として 考える の です 。 たとえば “ not amazing ” など の 否定 が 文章 内 に 含ま れ て いる か どう か を 検出 する ため の もの 、 という 感じ です 。 もし こんな フレーズ が 文章 の どこ か に で て き た 場合 、 その 部分 に フィルタ を 適用 する と 、 出力 さ れる 計算 結果 は 大きな 値 に なる でしょ う 。 しかし 、 それ 以外 の 別 の 部分 に フィルタ を 適用 し た 場合 は 、 出力 結果 は 小さな 値 に なる でしょ う 。 最大 プーリング を 適用 する こと で 、 文章 中 に “ とある 特徴 ” が 存在 する か どう か という 情報 は 残っ た まま です が 、 その 特徴 が 実際 に どこ に 出現 する の か といった 情報 は 失わ れる こと に なり ます 。 しかし 、 この よう な 位置 に関する 情報 は 本当に 消え て も 大丈夫 な の でしょ う か ？ 答え は yes です 。 n - grams モデル も 似 た よう な もの です 。 位置 に関する 情報 は 失い ます が 、 フィルタ によって 捉え られ た 局所 的 な 情報 - たとえば “ not amazing ” と “ amazing not ” の 違い など - は 残っ た まま な の です 。 で は 、 主 に 感情 分析 と カテゴリ 分類 から 成る 様々 な データセット 上 で CNN の アーキテクチャ を 評価 し て い ます 。 CNN は 全体 的 に とても 良い パフォーマンス を 発揮 し て い ます 。 この 論文 で 使わ れ て いる ネットワーク は 非常 に シンプル な のに 、 とても 強力 な ので 驚き です 。 入力 層 は word 2 vec による 単語 埋め込み 表現 で 構成 さ れ た 文章 で 、 その後 に 複数 の フィルタ を 持つ 畳み込み 層 と 最大 プーリング 層 が 続き 、 そして 最後 に softmax 分類 器 が あり ます 。 この 論文 で は 、 一方 は 学習 中 に 微 修正 さ れ て いく 動的 な 単語 埋め込み 表現 、 もう 一方 は 学習 中 に 変化 し ない 静的 な 単語 埋め込み 表現 、 といった 2 つ の 異なる チャンネル を 持つ データ に対する 実験 も し て い ます 。 似 て はい ます が 、 もう少し 複雑 な アーキテクチャ が [ 2 ] で 提案 さ れ て い ます 。 [ 6 ] で は “ semantic clustering ” と 呼ば れる 操作 を 行う 層 を ネットワーク に 追加 し て い ます 。 CNN を 実装 する ため に は 、 いろいろ な ハイパーパラメータ を 決める 必要 が あり ます 。 いくつ か は 先ほど 紹介 し まし た が 、 入力 データ の ベクトル 表現 ( word 2 vec な の か GloVe な の か one - hot な の か )、 その 数 、 畳み込み フィルタ の サイズ 、 プーリング の 方法 ( 最大 プーリング な の か 平均 プーリング な の か )、 活性 化 関数 ( ReLU な の か tanh な の か )、 など です 。 [ 7 ] で は CNN の ハイパーパラメータ を 様々 に 変化 さ せ ながら 、 その 時 の CNN の パフォーマンス と 複数 回 実行 し た 際 の 分散 を 調査 し 、 評価 し て い ます 。 もし 、 あなた が 自分 で テキスト 分類 問題 を 解く ため の CNN を 実装 する つもり で あれ ば 、 この 論文 の 結果 を 初期 値 として 使う の が 良い でしょ う 。 この 論文 に よる と 、 平均 プーリング より 最大 プーリング の 方 が 毎回 いい 結果 を 出し て おり 、 理想 的 な フィルタ サイズ を 考える の は とても 重要 です が それ は タスク 毎 に 異なっ て い ます 。 それから 、 正則 化 項 を 導入 し て も NLP において は 結果 が 大きく 変わる こと は ない よう です 。 1 つ 注意 点 として は 、 この 研究 で 使っ て いる データセット は どれ も テキスト の 長 さ が 非常 に 似 て いる もの ばかり です ので 、 テキスト 長 が 明らか に 異なる よう な データ に対して は 同じ よう に 考える こと は 恐らく でき ない でしょ う 。
自然 言語 処理 における 畳み込み ニューラルネットワーク を 理解 する 
最近 、 畳み込み ニューラルネットワーク を 使っ た テキスト 分類 の 実験 を し て い て 、 知見 が 溜まっ て き た ので それ について 何 か 記事 を 書こ う と 思っ て い た 時 に 、 こんな 記事 を みつけ まし た 。 
畳み込み ニューラルネットワーク を 自然 言語 処理 に 適用 する 話 な の です が 、 この 記事 、 個人 的 に わかり やすい な と 思っ た ので 、 著者 に 許可 を もらっ て 日本語 に 翻訳 し まし た 。 なお 、 この 記事 を 読む にあたって は 、 ニューラルネットワーク に関する 基礎 知識 程度 は 必要 か と 思わ れ ます 。 
※ 日本語 として より わかり やすく 自然 に なる よう に 、 原文 を 直訳 し て い ない 箇所 も いくつ か あり ます ので ご 了承 ください 。 翻訳 の 致命 的 な ミス など あり まし たら 、 Twitter など で 指摘 いただけれ ば すみやか に 修正 し ます 。 
以下 訳文 
畳み込み ニューラルネットワーク ( CNN ) という 言葉 を 聞い た 時 、 普通 は コンピューター ビジョン の こと を 思い浮かべる でしょ う 。 CNN は 過去 に 画像 分類 の 分野 において ブレーク スルー を 引き起こし 、 今日 で は Facebook の 写真 の 自動 タギング から 自動 運転 車 に 至る まで 、 ほとんど の コンピュータービジョンシステム の 中核 と なっ て い ます 。 
そして 近年 、 自然 言語 処理 ( NLP ) の 領域 の 問題 に対して も CNN が 適用 さ れ はじめ 、 いくつ か 興味深い 結果 を 得 て い ます 。 この 記事 で は CNN と は いったい 何 な の か という こと 、 また どの よう に し て NLP の 領域 で 使わ れる の か 、 という こと を 説明 し て み たい と 思い ます 。 コンピューター ビジョン で の ユース ケース を 話し た 方 が CNN において は いくぶん 直感 的 です ので 、 まずは そこ から 始め たい と 思い ます 。 そして ゆっくり と NLP の 話題 へ 移っ て いき ましょ う 。 
畳み込み と は ？ 
私 は 、 畳み込み について は 、 行列 に 適用 さ れる スライド 窓 関数 ( sliding window function ) として 考える と わかり やすい と 思い ます 。 言葉 で こう 書く と ちょっと 難しい かも しれ ませ ん が 、 視覚 的 に 表し て みる と 非常 に わかり やすい です 。 
左側 に ある 行列 は 白黒 の 画像 を 表し て いる と 考え て ください 。 行列 の 各 要素 は それぞれ 画像 の 1 つ の ピクセル に 対応 し て おり 、 0 が 黒 、 1 が 白 です ( 一般 的 に は 0 から 255 の 間 の 値 を 取る グレー スケール の 画像 )。 スライド 窓 は カーネル ( kernel ) や フィルタ ( filter ) または 特徴 検出 器 ( feature detector ) など と 呼ば れ ます 。 ここ で は 3 x 3 の フィルタ を 使っ て おり 、 その フィルタ の 値 と 行列 の 値 を 要素 毎 に 掛け あわ せ 、 それら の 値 を 合計 し ます 。 この 操作 を 、 行列 全体 を カバー する よう に フィルタ を スライド さ せ ながら 各 要素 に対して 行っ て いき 、 全体 の 畳み込み を 取得 し ます 。 
でも 結局 これ で 何 が 出来る の ？ と 不思議 に 思い ませ ん か 。 ここ で 直感 的 な 例 を 挙げ て み ましょ う 。 
各 ピクセル と その 周囲 を 平均 し て 画像 を ぼかす : 
各 ピクセル と その 周囲 の 差分 を とっ て エッジ を 検出 する : 
( これ を 直感 的 に 理解 する ため に は 、 ピクセル の 色 が 周囲 の 色 と 同じ で ある よう な 色 の 変化 が なめらか な 部分 で 、 どういう こと が 起こる の か を 考え て み ましょ う 。 そういった 部分 に この フィルタ を 適用 する と 、 可算 が 相殺 さ れ て 結果 的 に は 0 、 つまり 黒 に なり ます 。 逆 に 、 明度 が 極端 に 違う エッジ の 部分 で あれ ば - これ は たとえば 白 から 黒 へ 移り変わっ て いる 部分 - において は 、 差分 が 大きく なり 結果 的 に は 白く なり ます 。 ) 
GIMP の マニュアル に は 、 ここ で 紹介 し た 以外 の サンプル が いくつ か 含ま れ て い ます 。 畳み込み について より 詳しく 理解 し たけれ ば Chris Olah の 記事 も 読ん で みる こと を オススメ し ます 。 
畳み込み ニューラルネットワーク と は ？ 
さて 、 これ で 畳み込み の 正体 は わかり まし た 。 しかし 、 CNN と は 一体 な ん でしょ う ？ CNN は 、 ReLU や tanh の よう な 非線形 な 活性 化 関数 を 通し た 、 いくつ か の 畳み込み の 層 の こと です 。 伝統 的 な 順 伝搬 型 ニューラルネットワーク で は 、 それぞれ の 入力 ニューロン は 次 の 層 の ニューロン に それぞれ 接続 さ れ て おり 、 これ は 全 結合 層 や アフィン 層 と も 呼ば れ ます 。 しかし CNN で は その よう な こと は せ ず に 、 ニューロン の 出力 を 計算 する の に 畳み込み を 使い ます 。 これ によって 、 入力 と なる ニューロン の ある 領域 が 、 それぞれ 対応 する 出力 の ニューロン に 接続 さ れ て いる よう な 、 局所 的 な 接続 が できる こと に なり ます 。 各層 は 別々 の 異なる フィルタ を 適用 し - これ は 一般 的 に は 100 〜 1000 程度 の 数 に なり ます が - それら を 結合 し ます 。 これ を プーリング 層 ( subsampling ) と 呼び ます 。 これ について は 後ほど 詳しく お話 し ます 。 CNN の 学習 フェーズ で は 、 解決 し たい タスク に 適応 できる よう に フィルタ の 値 を 自動的 に 学習 し て いき ます 。 たとえば 画像 分類 の 話 で いう と 、 CNN は 最初 の 層 で 生 の ピクセル データ から エッジ を 検出 する ため の 学習 を 進め 、 その エッジ を 使っ て 今度 は 次 の 層 で 単純 な 形状 を 検出 し 、 さらに より 深い 層 で は その 形状 を 使っ て より 高 レベル な 特徴 、 つまり 顔 の 形状 など の 特徴 を 検出 する よう に なり ます 。 そして 最後 の 層 は 、 そういった 高 レベル な 特徴 を 使っ た 分類 器 と なり ます 。 
さて 、 畳み込み 計算 に は 注目 に 値す べき 点 が 2 つ あり ます 。 位置 不変 性 ( Location Invariance ) と 構成 性 ( Compositionality ) です 。 画像 の 中 に 象 が 写っ て いる か どう か を 判別 し たい と し ましょ う 。 畳み込み 層 で は 画像 全体 にわたって フィルタ を スライド さ せ て いく ので 、 画像 中 の どこ に 象 が 現れる の か を 気 に し なく て も よい の です 。 また プーリング 層 で も 、 平行 移動 、 回転 、 スケーリング に対して 不変 性 を 得る こと が でき ます が 、 これ は 後述 し ます 。 そして もう 1 つ の 鍵 は 構成 性 です 。 各 フィルタ は 、 低 レベル な 特徴 で ある 画像 の 一 区画 から 、 より 高 レベル な 特徴 を 表現 できる よう に し て くれ ます 。 これ が コンピューター ビジョン において CNN が 非常 に 強力 で ある 理由 です 。 ピクセル から エッジ を 、 エッジ から 形状 を 、 そして その 形状 から より 複雑 な オブジェクト を 構築 する 、 という 流れ は 直感 的 に 理 に かなっ て い ます 。 
これら を どう やっ て NLP へ 適用 する の か ？ 
画像 分類 で は 入力 は 画像 の ピクセル 列 に なり ます が 、 ほとんど の NLP タスク で は ピクセル 列 の 代わり に 、 行列 で 表現 さ れ た 文章 または 文書 が 入力 と なり ます 。 行列 の 各行 は 1 つ の トー クン に 対応 し て おり 、 一般 的 に は 単語 が トー クン に なる こと が 多い です が 、 文字 が トー クン でも かまい ませ ん 。 すなわち 、 各行 は 単語 を 表現 する ベクトル です 。 普通 、 これら の ベクトル は word 2 vec や GloVe の よう な 低 次元 な 単語 埋め込み 表現 ( word embeddings ) を 使い ます が 、 one - hot ベクトル でも かまい ませ ん 。 100 次元 の 単語 埋め 込み を 使っ た 10 単語 の 文章 が あっ た 場合 、 10 x 100 の 行列 と なり ます 。 これ が NLP における “ 画像 ” です 。 
コンピューター ビジョン で は 、 フィルタ は 画像 の ある 区画 上 を スライド し て いき ます が 、 NLP で は 一般 的 に 行列 の 行 全体 ( つまり 単語 毎 ) を スライド する フィルタ を 使い ます 。 つまり 、 フィルタ の 幅 は 入力 と なる 行列 の 幅 と 同じ に し ます 。 高 さ は 様々 です が 、 一般 的 に は 2 - 5 くらい の 単語 くらい でしょ う か 。 これら の こと を 加味 する と NLP の 畳み込み ニューラルネットワーク は こんな 感じ に なり ます ( 少し 時間 を かけ て この 図 を 見 て 、 どんな 風 に 計算 さ れ て いく の か を 理解 し て み て ください 。 プーリング について は 今 の ところ は 無視 し て ください 、 後で ちゃんと 説明 し ます )。 
文章 分類 の ため の 畳み込み ニューラルネットワーク ( CNN ) の アーキテクチャ を 説明 し た 図 。 この 図 に は 2 、 3 、 4 の 高 さ を もっ た フィルタ が 、 それぞれ 2 つ ずつ あり ます 。 各 フィルタ は 文章 の 行列 上 で 畳み込み を 行い 、 特徴 マップ を 生成 し ます 。 それから 、 各 特徴 マップ に対して 最大 プーリング を かけ て いき 、 各 特徴 マップ の 中 で 一番 大きい 値 を 記録 し て いき ます 。 そして 、 全 6 つ の 特徴 マップ から 単 変量 な 特徴 ( univariate feature ) が 生成 さ れ 、 それら 6 つ の 特徴 は 結合 さ れ て 、 それ が 最後 から 2 番目 の 層 に なり ます 。 一番 最後 の softmax 層 で は 先程 の 特徴 を 入力 として 受け取り 、 文章 を 分類 し ます 。 ここ で は 二 値 分類 を 前提 と し て い ます ので 、 最終 的 に は 2 つ の 出力 が あり ます 。 
引用 元 : hang , Y ., & Wallace , B . ( 2015 ). A Sensitivity Analysis of ( and Practitioners ’ Guide to ) Convolutional Neural Networks for Sentence Classification 
位置 不変 性 と 構成 性 は 、 画像 において は 直感 的 に わかり ます が 、 NLP の 場合 は そう で は あり ませ ん 。 NLP で は たぶん 、 文章 内 で 単語 が 出現 する 場所 なんか を 気 に する ん じゃ ない でしょ う か 。 お互い に 近く に ある ピクセル 同士 は 意味 的 に 関連 し て いる 、 つまり は 同じ オブジェクト で ある と 言える と 思い ます が 、 単語 において それ は 常に そう と は 限り ませ ん 。 多く の 言語 において 、 フレーズ の 一部 は 単語 によって 区切る こと が でき ます 。 こう いっ た 文章 の 組成 は 明確 な もの で は あり ませ ん 。 単語 の より 高 レベル な 表現 ( たとえば 実際 の 単語 の “ 意味 ” など ) は ピュータービジョン ほど 明らか で は なく 、 一体 この NLP に対する CNN は どう やっ て 動作 し て いる の でしょ う か 。 
こう なる と 、 CNN は NLP の タスク に は うまく 適合 し ない ん じゃ ない か という 気 が し て き ます 。 リカレントニューラルネットワーク は もっと 直感 的 です し 、 実際 に 人間 が 言語 を 処理 する やり方 に 似 て い ます 。 左 から 右 に 順番 に 読ん で いく 方法 です ね 。 と は 言っ て も 、 これ まで の 説明 は 決して CNN が うまく 動か ない という 意味 で は あり ませ ん 。 完璧 な モデル など 無い が 、 それでも 役に立つ モデル は ある という 言葉 が あり ます が 、 NLP の 問題 へ 適用 さ れ た CNN は 、 結果 的 に は とても よい 性能 を 発揮 し ます 。 単純 な Bag of Words モデル は 誤っ た 仮定 の もと 単純 化 し すぎ な の は 明らか です が 、 に も かかわら ず しばらく は 一般 的 な アプローチ で あり 、 人々 は それ を 使っ て より 良い 結果 を 得よ う と し て き まし た 。 
CNN を 使う 言い分 として は 、 とても 速い という こと です 。 非常 に 速い です 。 畳み込み は コンピューターグラフィックス において 重要 な もの で あり GPU 上 に ハードウェア レベル で 実装 さ れ て い ます 。 n - grams の よう な もの と 比べ て CNN は 効率 的 に 単語 を 表現 が でき ます 。 ボキャブラリー が 巨大 な 場合 、 3 - grams 以上 の もの は 計算 量 が 多 すぎ て すぐ に 計算 でき なく なり ます 。 Google で さえ 5 - grams を 超える もの は 提供 し て い ませ ん 。 畳み込み フィルタ は ボキャブラリー 全体 を 表現 する 必要 なく 、 勝手 に 適切 な 表現 を 学習 し て くれ ます 。 最初 の 層 に ある たくさん の 学習 済み の フィルタ は n - grams と 非常 に 良く 似 た 特徴 を 捉え ます が 、 より コンパクト な 表現 を 得る こと が できる と 考える と 良い でしょ う ( しかも 計算 量 が 多 すぎ て 計算 が でき ない といった 制限 も あり ませ ん )。 
CNN の ハイパーパラメータ 
CNN を NLP へ 適用 する やり方 を 説明 する 前 に 、 CNN を 構築 する 際 に 必要 と なっ て くる もの が いくつ か あり ます ので それ を 見 て いき ましょ う 。 きっと CNN 理解 の 手助け と なる はず です 。 
畳み込み 幅 の サイズ 
私 が 最初 に 畳み込み の 説明 を し た 時 、 フィルタ を 適用 する 際 の 詳細 について 説明 を 飛ばし た もの が あり ます 。 行列 の 真ん中 辺り に 3 x 3 の フィルタ を 適用 する の は 問題 あり ませ ん が 、 それでは フチ の 辺り に 適用 する 場合 は どう でしょ う か ？ 行列 の 左側 に も 上側 に も 隣接 し た 要素 が ない よう な 、 たとえば 行列 の 最初 の 要素 に は どう やっ て フィルタ を 適用 すれ ば よい でしょ う か ？ そういった 場合 に は 、 ゼロパディング が 使え ます 。 行列 の 外側 に はみ出し て しまう 要素 は 全て 0 で 埋める の です 。 こう する こと で 、 入力 と なる 行列 の 全 要素 にわたって フィルタ を 適用 する こと が でき ます 。 ゼロパディング を 行う こと は wide convolution と も 呼ば れ 、 逆 に ゼロパディング を し ない 場合 は narrow convolution と 呼ば れ ます 。 1 次元 で の 例 を 見 て み ましょ う 。 
入力 データ の サイズ に対して フィルタ サイズ が 大きい 時 に は wide convolution が 有用 です 。 narrow convolution は 出力 さ れる サイズ が ( 7 − 5 )+ 1 = 3 ( 7 − 5 )+ 1 = 3 に なり ます し 、 wide convolutin は ( 7 + 2 ∗ 4 − 5 )+ 1 = 11 ( 7 + 2 ∗ 4 − 5 )+ 1 = 11 に なり ます 。 一般 化 する と 、 出力 サイズ は nout =( nin + 2 ∗ npadding − nfilter )+ 1 nout =( nin + 2 ∗ npadding − nfilter )+ 1 です 。 
ストライド 
ストライド という 畳み込み のも う 一つ の ハイパーパラメータ が あり ます 。 これ は フィルタ を 順に 適用 し て いく 際 に 、 フィルタ を どれ くらい シフト する の か という 値 です 。 これ まで に 示し て き た 例 は 全て ストライド 1 で 、 フィルタ は 重複 し ながら 連続 的 に 適用 さ れ て い ます 。 ストライド を 大きく する と フィルタ の 適用 回数 は 少なく なっ て 、 出力 の サイズ も 小さく なり ます 。 以下 の よう な 図 が Stanford cs 231 に あり ます が 、 これ は 1 次元 の 入力 に対して 、 ストライド の サイズ が 1 または 2 の フィルタ を 適用 し て いる 様子 です 。 
畳み込み の ストライド の サイズ 。 左側 の ストライド は 1 。 右側 の ストライド は 2 。 引用 元 : http :// cs 231 n . github . io / convolutional - networks / 
普通 、 文書 において は ストライド の サイズ は 1 です が 、 ストライド の サイズ を 大きく する こと で 、 例えば ツリー の よう な 再帰 型 ニューラルネットワーク と 似 た 挙動 を 示す モデル を 作れる かも しれ ませ ん 。 
プーリング 層 
畳み込み ニューラルネットワーク の 鍵 は 、 畳み込み 層 の 後 に 適用 さ れる プーリング 層 です 。 プーリング 層 は 、 入力 を サブ サンプリング し ます 。 最も 良く 使わ れる プーリング は 、 各 フィルタ の 結果 の 中 から 最大 値 を 得る 操作 です 。 ただ 、 畳み込み 結果 の 行列 全体 にわたって プーリング する 必要 は なく 、 指定 サイズ の ウィンドウ 上 で プーリング する こと も でき ます 。 たとえば 、 以下 の 図 は 2 x 2 の サイズ の ウィンドウ 上 で 最大 プーリング を 実行 し た 様子 です ( NLP で は 一般 的 に 出力 全体 にわたって プーリング を 適用 し ます 。 つまり 各 フィルタ から は 1 つ の 数値 が 出力 さ れる こと に なり ます )。 
プーリング 層 を はさむ 理由 は いくつ か あり ます 。 プーリング の 特徴 の 1 つ は 、 出力 さ れる 行列 が 固定 サイズ に なる という こと です 。 たとえば 1000 個 の フィルタ が あっ て それぞれ の フィルタ に対して 最大 プーリング を 適用 し た と する と 、 入力 の サイズ や フィルタ の サイズ が どんな もの で あっ て も 結果 として は 1000 次元 の 出力 が 得 られ ます ね 。 これ は つまり 、 文章 の サイズ や フィルタ の サイズ が 可変 だっ た として も 、 最終 的 に 分類 器 へ データ が 渡っ て くる 時点 で は 常に 同じ 次元 に なっ て いる という こと です 。 
また 、 プーリング は 次元 削減 も 行い ます が 、 単に 次元 を 削減 する の で は なく 必要 な 情報 は 維持 し た まま 次元 を 削減 し て くれ ます 。 フィルタ を ある 特定 の 特徴 を 抽出 する ため の もの として 考える の です 。 たとえば “ not amazing ” など の 否定 が 文章 内 に 含ま れ て いる か どう か を 検出 する ため の もの 、 という 感じ です 。 もし こんな フレーズ が 文章 の どこ か に で て き た 場合 、 その 部分 に フィルタ を 適用 する と 、 出力 さ れる 計算 結果 は 大きな 値 に なる でしょ う 。 しかし 、 それ 以外 の 別 の 部分 に フィルタ を 適用 し た 場合 は 、 出力 結果 は 小さな 値 に なる でしょ う 。 最大 プーリング を 適用 する こと で 、 文章 中 に “ とある 特徴 ” が 存在 する か どう か という 情報 は 残っ た まま です が 、 その 特徴 が 実際 に どこ に 出現 する の か といった 情報 は 失わ れる こと に なり ます 。 しかし 、 この よう な 位置 に関する 情報 は 本当に 消え て も 大丈夫 な の でしょ う か ？ 答え は yes です 。 n - grams モデル も 似 た よう な もの です 。 位置 に関する 情報 は 失い ます が 、 フィルタ によって 捉え られ た 局所 的 な 情報 - たとえば “ not amazing ” と “ amazing not ” の 違い など - は 残っ た まま な の です 。 
画像 認識 で の 話 です が 、 プーリング は 位置 と 回転 に 不変 性 を 与え ます 。 ある 領域 について プーリング を 行う と 、 画像 が 数 ピクセル だけ 移動 や 回転 を し て も その 出力 は ほぼ 同じ に なり ます 。 それ は 最大 プーリング が 、 微妙 な ピクセル の 違い を 無視 し て 同じ 値 を 抽出 し て き て くれる から です 。 
チャンネル 
最後 は チャンネル です 。 チャンネル と は 、 入力 データ を 異なる 視点 から 見 た もの と 言える でしょ う 。 画像 認識 で の 例 を 挙げる と 、 普通 は 画像 は RGB ( red , green , blue ) の 3 チャンネル を 持っ て い ます 。 畳み込み は これら の チャンネル 全体 に 適用 でき 、 その 時 の フィルタ は 各 チャンネル 毎 に 別々 に 用意 し て も いい です し 、 同じ もの を 使っ て も かまい ませ ん 。 NLP で は 、 異なる 単語 埋め込み 表現 ( word 2 vec や GloVe など ) で チャンネル を 分け たり 、 同じ 文章 を 異なる 言語 で 表現 し て み たり 、 また 異なる フレーズ で 表現 し て み たり 、 という 風 に し て 複数 チャンネル を 持た せる こと が でき そう です ね 。 
NLP へ 適用 さ れ た 畳み込み ニューラルネットワーク 
それでは 、 自然 言語 処理 に対して CNN を 使っ た アプリケーション の 研究 結果 について 見 て いき ましょ う 。 CNN の アプリケーション について は 私 が 知ら ない もの も たくさん あり ます が 、 少なくとも 有名 な 研究 結果 について は カバー でき て いる と 思い ます 。 
CNN が 得意 な の は 、 感情 分析 ( Sentiment Analysis ) や スパム 検出 ( Spam Detection )、 カテゴリ 分類 ( Topic Categorization ) など の 分類 問題 です 。 畳み込み と プーリング の 操作 を 適用 する と 、 単語 の 局所 的 な 位置 情報 は 失わ れ ます ので 、 品詞 タグ 付け ( PoS Tagging ) や 固有 表現 抽出 ( Entity Extraction ) など を 目的 として 、 純粋 な CNN を 使う の は ちょっと 難しい でしょ う ( ただ 、 入力 データ に 位置 情報 に関する 特徴 を 追加 すれ ば 不可能 で は ない と 思い ます )。 
[ 1 ] で は 、 主 に 感情 分析 と カテゴリ 分類 から 成る 様々 な データセット 上 で CNN の アーキテクチャ を 評価 し て い ます 。 CNN は 全体 的 に とても 良い パフォーマンス を 発揮 し て い ます 。 この 論文 で 使わ れ て いる ネットワーク は 非常 に シンプル な のに 、 とても 強力 な ので 驚き です 。 入力 層 は word 2 vec による 単語 埋め込み 表現 で 構成 さ れ た 文章 で 、 その後 に 複数 の フィルタ を 持つ 畳み込み 層 と 最大 プーリング 層 が 続き 、 そして 最後 に softmax 分類 器 が あり ます 。 この 論文 で は 、 一方 は 学習 中 に 微 修正 さ れ て いく 動的 な 単語 埋め込み 表現 、 もう 一方 は 学習 中 に 変化 し ない 静的 な 単語 埋め込み 表現 、 といった 2 つ の 異なる チャンネル を 持つ データ に対する 実験 も し て い ます 。 似 て はい ます が 、 もう少し 複雑 な アーキテクチャ が [ 2 ] で 提案 さ れ て い ます 。 [ 6 ] で は “ semantic clustering ” と 呼ば れる 操作 を 行う 層 を ネットワーク に 追加 し て い ます 。 
Kim , Y . ( 2014 ). Convolutional Neural Networks for Sentence Classification 
[ 4 ] で は 、 word 2 vec や GloVe など を 使っ た 単語 ベクトル の 事前 学習 は せ ず に 、 one - hot な ベクトル に対して 直接 畳み込み を 適用 し て 、 スクラッチ で CNN を 学習 さ せ て い ます 。 著者 は また 、 空間 効率 の 良い ( space - efficient ) BoW の よう な 表現 を 提案 し て おり 、 ネットワーク が 学習 する パラメータ 数 を 減らし て い ます 。 [ 5 ] で は 、 CNN を 使っ て テキスト の ある 領域 の コンテキスト を 予測 する “ region embedding ” と 呼ば れる 教師 なし の モデル を 拡張 し て い ます 。 これら の 論文 の アプローチ は 長い テキスト ( たとえば 映画 の レビュー の よう な ) に対して は うまく 動作 し て いる よう に 見え ます が 、 短い テキスト ( ツイート など ) に対して は そう で は あり ませ ん 。 短い テキスト に対して は 、 単語 埋め込み 表現 を 事前 学習 し て おい た 方 が より 良い 結果 に なり そう です ね 。 
CNN を 実装 する ため に は 、 いろいろ な ハイパーパラメータ を 決める 必要 が あり ます 。 いくつ か は 先ほど 紹介 し まし た が 、 入力 データ の ベクトル 表現 ( word 2 vec な の か GloVe な の か one - hot な の か )、 その 数 、 畳み込み フィルタ の サイズ 、 プーリング の 方法 ( 最大 プーリング な の か 平均 プーリング な の か )、 活性 化 関数 ( ReLU な の か tanh な の か )、 など です 。 [ 7 ] で は CNN の ハイパーパラメータ を 様々 に 変化 さ せ ながら 、 その 時 の CNN の パフォーマンス と 複数 回 実行 し た 際 の 分散 を 調査 し 、 評価 し て い ます 。 もし 、 あなた が 自分 で テキスト 分類 問題 を 解く ため の CNN を 実装 する つもり で あれ ば 、 この 論文 の 結果 を 初期 値 として 使う の が 良い でしょ う 。 この 論文 に よる と 、 平均 プーリング より 最大 プーリング の 方 が 毎回 いい 結果 を 出し て おり 、 理想 的 な フィルタ サイズ を 考える の は とても 重要 です が それ は タスク 毎 に 異なっ て い ます 。 それから 、 正則 化 項 を 導入 し て も NLP において は 結果 が 大きく 変わる こと は ない よう です 。 1 つ 注意 点 として は 、 この 研究 で 使っ て いる データセット は どれ も テキスト の 長 さ が 非常 に 似 て いる もの ばかり です ので 、 テキスト 長 が 明らか に 異なる よう な データ に対して は 同じ よう に 考える こと は 恐らく でき ない でしょ う 。 
[ 8 ] で は 関係 抽出 ( Relation Extraction ) と 関係 分類 ( Relation Classification ) に対して CNN の 研究 を し て い ます 。 単語 ベクトル に関して 、 興味 の ある エンティティ に対する 単語 の 相対 位置 を 畳み込み 層 へ の 入力 として 使っ て い ます 。 この モデル は 、 エンティティ の 位置 は あらかじめ 与え られ て いる もの として 、 各 サンプル データ は それぞれ 1 つ の 関連 を 含ん で い ます 。 [ 9 ] と [ 10 ] で も 似 た よう な モデル を 使っ て い ます 。 
他 の 面白 そう な ユース ケース として は Microsoft Research から 発表 さ れ た [ 11 ] と [ 12 ] が あり ます 。 これら の 論文 は 、 情報 検索 システム で 使わ れる 文章 の “ 意味 に関する 有用 な 表現 ” を どう やっ て 学習 する の か を 説明 し て い ます 。 ユーザー が 現在 読ん で いる テキスト を ベース として 、 ユーザー が 潜在 的 に 興味 が ある で あろ う 文書 を オススメ する 手法 が 例 として 含ま れ て い ます 。 この 文書 表現 は 検索 エンジン の ログ データ を 元 に 学習 さ れ ます 。 
ほとんど の CNN アーキテクチャ は 、 学習 プロセス の 一部 として 単語 及び 文章 に対して の 埋め込み 表現 ( より 低 次元 な 表現 ) を 学習 し ます 。 すべて の 論文 が この やり方 に 沿っ て いる わけ で は あり ませ ん し 、 有用 な 埋め込み 表現 を どの よう に 学習 する の か という 調査 を し て いる わけ で も あり ませ ん 。 [ 13 ] で は 、 単語 や 文章 に対する 埋め込み 表現 を 生成 し ながら Facebook へ の 投稿 の ハッシュ タグ を 予測 する CNN を 紹介 し て い ます 。 こう いっ た 学習 済み の 埋め込み 表現 は 、 他 の タスク に 適用 し て も うまく いき ます 。 
キャラクター レベル CNN 
ここ まで は 、 どの モデル も 単語 を ベース に し た もの でし た 。 しかし 文字 に対して 直接 CNN を 適用 する 研究 も 続け られ て き まし た 。 [ 14 ] で は 文字 レベル ( character - level ) で 埋め込み 表現 を 学習 し 、 それら を 事前 学習 し た 単語 埋め込み 表現 と 結合 し 、 品詞 タグ 付け を する ため に CNN を 使っ て い ます 。 [ 15 ] と [ 16 ] で は 、 事前 学習 さ れ た 埋め込み 表現 を 使わ ず に 、 文字 から 直接 学習 する CNN を 説明 し て い ます 。 とりわけ 著者 は 、 全部 で 9 個 の 層 を 持つ 比較的 深い ネットワーク を 感情 分析 や テキスト 分類 に 適用 し て い ます 。 大 規模 な データセット ( 100 万 件 程度 の データ ) を 使え ば 、 文字 レベル の 入力 を 直接 学習 する こと で 良い 結果 が 得 られる こと が わかっ て い ます が 、 小規模 な データセット ( 数 千 件 程度 の データ ) しか ない 場合 は 単純 な モデル に も 負け て しまい ます 。 [ 17 ] で は 文字 レベル の CNN の 出力 を LSTM へ の 入力 として 使う よう な アプリケーション に対する 説明 を し て い ます 。 これ は 様々 な 言語 に 適用 でき ます 。 
驚く べき は 、 これら の 論文 は 全て 過去 1 - 2 年 の 間 で 公開 さ れ た 論文 だ という こと です 。 NLP に CNN を 適用 する 以前 から 、 自然 言語 処理 界隈 で は CNN を 使わ ず に スクラッチ で 素晴らしい 成果 を 出し て き まし た が 、 最近 の 新しい 結果 や 最新 の システム が 公開 さ れる スピード は 加速 し 続け て い ます ね 。 

