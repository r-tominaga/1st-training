# coding: UTF-8
import MeCab
mt = MeCab.Tagger("-Owakati")
print(mt.parse("こうなると、CNN は NLP のタスクにはうまく適合しないんじゃないかという気がしてきます。リカレントニューラルネットワーク はもっと直感的ですし、実際に人間が言語を処理するやり方に似ています。左から右に順番に読んでいく方法ですね。とは言っても、これまでの説明は決して CNN がうまく動かないという意味ではありません。完璧なモデルなど無いが、それでも役に立つモデルはある という言葉がありますが、NLP の問題へ適用された CNN は、結果的にはとてもよい性能を発揮します。単純な Bag of Words モデル は誤った仮定のもと単純化しすぎなのは明らかですが、にもかかわらずしばらくは一般的なアプローチであり、人々はそれを使ってより良い結果を得ようとしてきました。CNN を使う言い分としては、とても速いということです。非常に速いです。畳み込みはコンピューターグラフィックスにおいて重要なものであり GPU 上にハードウェアレベルで実装されています。n-grams のようなものと比べて CNN は効率的に単語を表現ができます。ボキャブラリーが巨大な場合、3-grams 以上のものは計算量が多すぎてすぐに計算できなくなります。Google でさえ 5-grams を超えるものは提供していません。畳み込みフィルタはボキャブラリー全体を表現する必要なく、勝手に適切な表現を学習してくれます。最初の層にあるたくさんの学習済みのフィルタは n-grams と非常に良く似た特徴を捉えますが、よりコンパクトな表現を得ることができると考えると良いでしょう (しかも計算量が多すぎて計算ができないといった制限もありません)。CNN を NLP へ適用するやり方を説明する前に、CNN を構築する際に必要となってくるものがいくつかありますのでそれを見ていきましょう。きっと CNN 理解の手助けとなるはずです。私が最初に畳み込みの説明をした時、フィルタを適用する際の詳細について説明を飛ばしたものがあります。行列の真ん中辺りに 3x3 のフィルタを適用するのは問題ありませんが、それではフチの辺りに適用する場合はどうでしょうか？行列の左側にも上側にも隣接した要素がないような、たとえば行列の最初の要素にはどうやってフィルタを適用すればよいでしょうか？そういった場合には、ゼロパディングが使えます。行列の外側にはみ出してしまう要素は全て 0 で埋めるのです。こうすることで、入力となる行列の全要素にわたってフィルタを適用することができます。ゼロパディングを行うことは wide convolution とも呼ばれ、逆にゼロパディングをしない場合は narrow convolution と呼ばれます。1 次元での例を見てみましょう入力データのサイズに対してフィルタサイズが大きい時には wide convolution が有用です。ストライドという畳み込みのもう一つのハイパーパラメータがあります。これはフィルタを順に適用していく際に、フィルタをどれくらいシフトするのかという値です。これまでに示してきた例は全てストライド 1 で、フィルタは重複しながら連続的に適用されています。ストライドを大きくするとフィルタの適用回数は少なくなって、出力のサイズも小さくなります。以下のような図が Stanford cs231 にありますが、これは 1 次元の入力に対して、ストライドのサイズが 1 または 2 のフィルタを適用している様子です。畳み込みニューラルネットワークの鍵は、畳み込み層の後に適用されるプーリング層です。プーリング層は、入力をサブサンプリングします。最も良く使われるプーリングは、各フィルタの結果の中から最大値を得る操作です。ただ、畳み込み結果の行列全体にわたってプーリングする必要はなく、指定サイズのウィンドウ上でプーリングすることもできます。たとえば、以下の図は 2x2 のサイズのウィンドウ上で最大プーリングを実行した様子です (NLP では一般的に出力全体にわたってプーリングを適用します。つまり各フィルタからは 1 つの数値が出力されることになります)。プーリング層をはさむ理由はいくつかあります。プーリングの特徴の 1 つは、出力される行列が固定サイズになるということです。たとえば 1000 個のフィルタがあってそれぞれのフィルタに対して最大プーリングを適用したとすると、入力のサイズやフィルタのサイズがどんなものであっても結果としては 1000 次元の出力が得られますね。これはつまり、文章のサイズやフィルタのサイズが可変だったとしても、最終的に分類器へデータが渡ってくる時点では常に同じ次元になっているということです。また、プーリングは次元削減も行いますが、単に次元を削減するのではなく必要な情報は維持したまま次元を削減してくれます。フィルタをある特定の特徴を抽出するためのものとして考えるのです。たとえば “not amazing” などの否定が文章内に含まれているかどうかを検出するためのもの、という感じです。もしこんなフレーズが文章のどこかにでてきた場合、その部分にフィルタを適用すると、出力される計算結果は大きな値になるでしょう。しかし、それ以外の別の部分にフィルタを適用した場合は、出力結果は小さな値になるでしょう。最大プーリングを適用することで、文章中に “とある特徴” が存在するかどうかという情報は残ったままですが、その特徴が実際にどこに出現するのかといった情報は失われることになります。しかし、このような位置に関する情報は本当に消えても大丈夫なのでしょうか？答えは yes です。n-grams モデルも似たようなものです。位置に関する情報は失いますが、フィルタによって捉えられた局所的な情報 - たとえば “not amazing” と “amazing not” の違いなど - は残ったままなのです。では、主に感情分析とカテゴリ分類から成る様々なデータセット上で CNN のアーキテクチャを評価しています。CNN は全体的にとても良いパフォーマンスを発揮しています。この論文で使われているネットワークは非常にシンプルなのに、とても強力なので驚きです。入力層は word2vec による単語埋め込み表現で構成された文章で、その後に複数のフィルタを持つ畳み込み層と最大プーリング層が続き、そして最後に softmax 分類器があります。この論文では、一方は学習中に微修正されていく動的な単語埋め込み表現、もう一方は学習中に変化しない静的な単語埋め込み表現、といった 2 つの異なるチャンネルを持つデータに対する実験もしています。似てはいますが、もう少し複雑なアーキテクチャが [2] で提案されています。[6] では “semantic clustering” と呼ばれる操作を行う層をネットワークに追加しています。CNN を実装するためには、いろいろなハイパーパラメータを決める必要があります。いくつかは先ほど紹介しましたが、入力データのベクトル表現 (word2vec なのか GloVe なのか one-hot なのか)、その数、畳み込みフィルタのサイズ、プーリングの方法 (最大プーリングなのか平均プーリングなのか)、活性化関数 (ReLU なのか tanh なのか)、などです。[7] では CNN のハイパーパラメータを様々に変化させながら、その時の CNN のパフォーマンスと複数回実行した際の分散を調査し、評価しています。もし、あなたが自分でテキスト分類問題を解くための CNN を実装するつもりであれば、この論文の結果を初期値として使うのが良いでしょう。この論文によると、平均プーリングより最大プーリングの方が毎回いい結果を出しており、理想的なフィルタサイズを考えるのはとても重要ですがそれはタスク毎に異なっています。それから、正則化項を導入しても NLP においては結果が大きく変わることはないようです。1 つ注意点としては、この研究で使っているデータセットはどれもテキストの長さが非常に似ているものばかりですので、テキスト長が明らかに異なるようなデータに対しては同じように考えることは恐らくできないでしょう。"))
